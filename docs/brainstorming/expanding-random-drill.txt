chris: I'm curious about thinking through the Random Drill mode in the App some more to train this further.

Right now, it has wikipedia-based content with Easy (simple english wikipedia Good/Very Good articles), Medium (graded easier wikipedia Good/Featured Articles), and Hard (full Good/Featured Article corpus without filtering). These are good for some purposes -- expanding one's horizons across the wikipedia corpus into subjects and facts one doesn't normally get exposed to -- but not so great at general linguistic fluency, because the factual nature of the articles can strain memory in ways unrelated to the linguistic fluency process (e.g., specific dates, or names in foreign languages, strain working memory above and beyond the difficulty of "just" reading, say, fiction). I'd like to think about adding an option beyond the wikipedia training corpii to rectify this gap, but I'm having difficulty thinking about how best to source this corpus.
---
opus:This is a really well-identified gap. You're essentially saying: Wikipedia trains factual-domain processing where the difficulty confound is *memory for arbitrary referents* (dates, names, technical terms), but what you actually want to train is the pure linguistic processing pipeline — syntax parsing, prediction across varied prose styles, and lexical access for a wide vocabulary. Fiction and literary nonfiction exercise that pipeline much more cleanly because the context builds cumulatively and the "difficulty" is genuinely linguistic (complex syntax, unusual word choice, longer dependency structures) rather than a working memory tax on arbitrary facts.

The sourcing problem is real though. Let me think through the options and their tradeoffs.

**Project Gutenberg** is the obvious first candidate — ~70,000 public domain works, freely redistributable, well-structured. The main problem is the temporal skew: most of the corpus is pre-1928 English, which means you're training on prose styles that are systematically different from modern English in ways that may not transfer well. Dickens-era syntax, Victorian vocabulary, and 19th-century essay conventions are linguistically rich but don't exercise the same prediction patterns you'd use reading contemporary prose. That said, for *training* purposes this might actually be a feature rather than a bug — if the goal is to build tolerance for varied syntactic structures, older prose forces you to process constructions you wouldn't predict from modern reading habits, which exercises exactly the flexibility you want.

**Standard Ebooks** is a curated, beautifully formatted subset of public domain works — around 800+ titles. The editorial quality is much higher than raw Gutenberg (cleaned typography, consistent formatting, good metadata). This could give you a "curated classic fiction" tier that's more reliable than pulling randomly from Gutenberg, where quality varies wildly.

**The grading problem** is harder for fiction than for Wikipedia. With Wikipedia you can use the article quality ratings as a proxy. For fiction, you'd need something else. A few approaches:

Readability metrics (Flesch-Kincaid, Coleman-Liau, etc.) are computationally cheap and would let you auto-grade excerpts. They're imperfect — Hemingway scores as "easy" despite being demanding in ways readability scores don't capture — but for a first pass at difficulty tiers, they'd work. You could compute scores per chapter or per N-paragraph window and bin them into your Easy/Medium/Hard tiers.

A more sophisticated approach would combine readability with vocabulary frequency profiles. Texts that use lots of low-frequency words are genuinely harder to process at speed, regardless of syntactic complexity. You could use something like a word frequency list (SUBTLEXus or similar) and compute the proportion of words below, say, the 10,000th-frequency rank as a difficulty signal.

**The excerpt problem** is the other design challenge. Wikipedia articles are relatively self-contained — you can drop into any article and the first paragraph orients you. Fiction isn't like that. A random excerpt from the middle of a novel lacks narrative context. But I'd argue this is actually acceptable for your purposes, and maybe even desirable. You're not training discourse-level comprehension with the random drill — you're training lexical access, syntactic parsing, and local prediction. For those purposes, mid-novel excerpts work fine because the sentence-level and paragraph-level processing is the same whether or not you know who the characters are. The slight disorientation of not having narrative context actually mimics the experience of reading in an unfamiliar domain, which is the generalized skill you're trying to build.

**Essays and literary nonfiction** might be an underappreciated middle ground. They're discursive prose (so they exercise the prediction pipeline well), they don't require narrative context (each essay is relatively self-contained), and they cover a huge range of styles and difficulties. Public domain sources include Montaigne, Emerson, Orwell's essays, Hazlitt, Lamb, Virginia Woolf's nonfiction — plus there are CC-licensed essay collections online. This would give you a corpus that's neither factual-encyclopedic nor narrative-dependent.

For practical implementation, I'd think about it as adding one or two new corpus tiers alongside the Wikipedia ones — maybe "Prose: Classic" (Gutenberg/Standard Ebooks fiction, auto-graded by readability) and "Prose: Essays" (public domain essay collections). The preprocessing pipeline would need to handle chapter/section extraction and compute readability scores for difficulty binning, but the basic structure would parallel what you already have for Wikipedia.

What's your preprocessing pipeline like currently for the Wikipedia content? That would help me think about what would be easy versus hard to bolt on.
----
chris: The excerpt problem, along with a standardized corpus -- project gutenberg is good, but stylistically extremely varied, which has its pros and cons depending on how we want to filter things. I'm thinking something like a representative 20th century+ English language material would be ideal. And then there's still the problem of excerpting. Dropping into novels at random would be quite jarring.
----
opus:You're right that the excerpt problem is the harder of the two, and it actually constrains the corpus choice more than the other way around. Let me think through this from the excerpt problem first and let that drive the corpus selection.

The cleanest solution to the excerpt problem is to use inherently self-contained units of prose rather than trying to excerpt from longer works. That points toward **short stories** as the ideal form. Each one is a complete unit — it establishes its own context, has a beginning, and doesn't require knowledge of anything outside itself. You can present a full story or a natural opening segment without any jarring drop-in effect.

This also happens to help with the 20th century+ constraint, though not in the way you might initially hope. The copyright situation is the real bind. As of 2026, US public domain covers works published before 1930, so you're getting early 20th century at best from that angle — Hemingway's early stories, Fitzgerald, Mansfield, Joyce's *Dubliners*, D.H. Lawrence, Woolf, O. Henry. That's good prose but still stylistically distant from contemporary English in some ways.

For genuinely modern prose, the most promising avenue I'm aware of is **Creative Commons-licensed literary magazines**, particularly in the SF/F space. Clarkesworld Magazine publishes under CC BY-NC-ND and has a deep archive — hundreds of stories going back to 2006, all freely available, all professionally edited contemporary English prose. The writing quality is high (multiple Hugo and Nebula winners in there), and while the content is speculative fiction, the *prose style* is modern literary English — varied sentence structures, contemporary vocabulary, the full range of difficulty from accessible to dense. Lightspeed Magazine similarly has freely available fiction. These solve every problem simultaneously: self-contained units, modern prose, legal to redistribute (check the specific license terms against your use case), and professionally edited to a consistent quality standard.

The genre skew is a legitimate concern — if your entire modern prose corpus is SF/F, you're training on a specific register. But I'd argue it's less of a problem than it might seem. The linguistic processing you're training — syntactic parsing, lexical access, prediction — operates below the genre level. A Clarkesworld story about first contact exercises the same language processing machinery as a New Yorker story about a failing marriage, at least at the sentence-to-paragraph scale where your training modes operate. The differences are more in vocabulary domain (which is actually useful — unfamiliar domain vocabulary is a feature for training) than in syntactic complexity or prose rhythm.

For a more balanced corpus you could layer several sources. Something like:

**Tier 1 — Early Modern (public domain, 1900-1930):** Curated short stories from Project Gutenberg and Standard Ebooks. Mansfield, Joyce's *Dubliners*, early Hemingway, Chekhov in good translation. These are stylistically more "literary" and syntactically more complex on average — could serve as your Hard tier.

**Tier 2 — Contemporary (CC-licensed):** Clarkesworld archive, Lightspeed free fiction, possibly other CC magazines. Modern prose rhythms, contemporary vocabulary. Could span Easy to Medium depending on the specific story.

**Tier 3 — Essays (public domain + CC):** Orwell's essays (entering public domain soon or recently), plus CC-licensed essay collections, lecture transcripts, etc. This exercises discursive rather than narrative prose — argument-following rather than story-following, which is a distinct prediction skill and probably closer to the academic reading you actually do.

For difficulty grading across all three tiers, you could compute a composite score from readability index plus vocabulary frequency profile (proportion of words outside the top N frequency ranks), applied uniformly regardless of source. That gives you consistent difficulty binning across genres and eras.

The preprocessing for short stories is also simpler than for novels — no chapter extraction needed, just story boundary detection. Clarkesworld's archive is well-structured HTML with clean story boundaries, so scraping and processing would be straightforward.

Does the short story approach resonate, or are you drawn toward a different unit of prose? And is the CC-licensed magazine route workable for your project's licensing situation?
----
chris: I think essay collections and short stories sound like an interesting avenue. Orwell, Mencken, Hemingway, authors like this may be within reach. Let's think this through more.
----
opus: Good — let me work through the practical availability and then how to structure it.

**What's actually in US public domain as of 2026.** The cutoff is works published before 1931. This is better than you might think for the authors you're naming:

*Hemingway* — **In Our Time** (1925), **Men Without Women** (1927), and **The Sun Also Rises** (1926) are all public domain. The short story collections are ideal: tightly crafted modern prose, varied sentence lengths, clean contemporary vocabulary. "Hills Like White Elephants," "The Killers," "Big Two-Hearted River" — these are linguistically demanding in exactly the right way (implication, compressed syntax, prediction based on tone rather than exposition).

*Mencken* — The entire **Prejudices** series (six volumes, 1919–1927), **A Book of Prefaces** (1917), **In Defense of Women** (1918), **The American Language** (1919 edition). This is a treasure trove for your purposes. Mencken's essays are self-contained, argumentative, syntactically complex, and use a huge vocabulary. Each essay runs 2,000–8,000 words, which is a natural training-session-sized unit. And his prose is stylistically much closer to modern essayistic English than most pre-1930 writing — punchy, opinionated, rhythmic.

*Orwell* — here's the bad news. His first published book was **Down and Out in Paris and London** (1933), so nothing is US public domain yet. You're looking at 2029 at the earliest. He's available in some other jurisdictions (life+70 entered public domain in 2021 in the UK/EU), but for a project you want to distribute cleanly, I'd set Orwell aside for now.

**Other strong candidates you might not have considered:**

*F. Scott Fitzgerald* — **Flappers and Philosophers** (1920), **Tales of the Jazz Age** (1922), **All the Sad Young Men** (1926). More stylistically varied than people remember — he ranges from tight, modern dialogue-driven stories to lush descriptive prose. Good difficulty spread.

*Dorothy Parker* — Her early New Yorker pieces and **Enough Rope** (1926). Sharp, modern-sounding, witty. Her prose has aged remarkably well stylistically — sentences that read like they could have been written yesterday.

*Virginia Woolf* — **The Common Reader** (1925) is a collection of literary essays that's perfect for your essay tier. And **Monday or Tuesday** (1921) for experimental short fiction. Woolf gives you the high end of syntactic complexity — long dependency structures, nested clauses, stream-of-consciousness passages that genuinely stress the parsing pipeline.

*Sherwood Anderson* — **Winesburg, Ohio** (1919). Interconnected short stories, each self-contained. Deceptively simple prose that's actually doing complex things with implication and repetition.

*Ring Lardner* — Short stories in vernacular American English. Interesting because the colloquial style exercises a different prediction pattern than literary prose — you're predicting based on speech rhythms rather than written conventions.

*Willa Cather* — **Youth and the Bright Medusa** (1920), plus novels pre-1930. Clean, muscular prose. Probably the closest to "transparent modern English" in the pre-1930 corpus.

**Corpus size reality check.** If you're training 15-30 minutes daily at your speeds (350-500 WPM), that's roughly 5,000–15,000 words per session, or 150,000–450,000 words per month. Hemingway's two story collections give you maybe 80,000 words. Mencken's Prejudices series is probably 300,000+. Fitzgerald's three story collections add another 150,000. Woolf's essays another 80,000. You can realistically assemble a corpus of 700,000–1,000,000 words from these authors alone, which would sustain several months of daily training without repetition. That's enough to be genuinely useful.

**How I'd structure the tiers for this corpus:**

Rather than Easy/Medium/Hard by author (which would be crude — Hemingway isn't "easier" than Woolf in any simple sense), I'd compute per-piece difficulty scores and let the grading cut across authors. A Hemingway story with dense dialogue and implication might score Medium, while a descriptive passage from Cather scores Easy, and a Woolf essay with 40-word sentences scores Hard. This lets you mix authors within each difficulty tier, which is better for training because you're not adapting to one writer's style — you're building generalized processing fluency.

The grading could combine average sentence length (proxy for syntactic complexity), vocabulary frequency profile (proportion of words outside the top 5,000/10,000 frequency ranks), and type-token ratio (lexical diversity). Those three signals together do a reasonable job of capturing what makes prose genuinely harder to process at speed.

**The excerpt structure for stories specifically:** For short stories, I'd present them whole rather than excerpting. Most of the stories in these collections are 2,000–8,000 words — that's a 5-20 minute training session at your speeds, which is a natural unit. For longer stories or the essay collections where pieces run longer, you could break at natural section boundaries or paragraph breaks after reaching a target word count, with a clean stopping point rather than a mid-sentence cutoff.

Want to dig into the technical side of sourcing and preprocessing these? Gutenberg and Standard Ebooks have different formats and quality levels, and getting clean text extraction right matters a lot for how the training experience feels.
